experiment:
  name: "TiViT-Piano_OMAPS_v1"
  seed: 1332 #It was 42
  output_dir: "outputs"

dataset:
  #name: "OMAPS"
  #root_dir: "~/datasets/OMAPS"           # adjust if needed
  #annotations_root: "~/datasets/OMAPS"   # adjust if needed
  name: "PianoYT"
  root_dir: "data/PianoYT"           # adjust if needed
  annotations_root: "data/PianoYT"   # adjust if needed
  label_format: "txt"
  label_targets: ["pitch", "onset", "offset", "hand", "clef"]
  max_clips: null
  # NEW: manifest files that select which IDs belong to each logical role
  manifest:
    #train: splits/train_minus_calib_list.txt  #For OMAPS
    #val:   splits/calib_list.txt  #For OMAPS
    train: null #data/PianoYT/splits/train_minus_val.txt  #For PianoYT
    val:   null #data/PianoYT/splits/splits/val.txt  #For PianoYT
    test:  null                         # test uses the whole test/ folder
  split: "train"
  split_train: "train"
  split_val: "val"
  split_test: "test"
  frames: 96
  # Time-grid parameters
  decode_fps: 30.0
  hop_seconds: 0.03333333333333333  # 1 / decode_fps
  resize: [128, 128]
  tiles: 3
  channels: 3
  normalize: true
  apply_crop: true
  canonical_hw: [145, 800]
  registration:
    source: "metadata"
    interp: "bilinear"
    global_aug:
      enabled: false
      resize_jitter: [150, 805]
      random_crop_hw: [145, 800]
  crop_rescale: "auto"
  include_low_res: false
  batch_size: 2
  num_workers: 4   
  shuffle: true
  require_labels: true
  frame_targets:
    enable: true
    tolerance: 0.03  
    dilate_active_frames: 1   
    fill_mode: "overlap"
    hand_from_pitch: true
    clef_thresholds: [60, 64]
    note_min: 0
    note_max: 127
    cache_labels: true
    cache_dir: ".cache/labels"
    targets_sparse: true #It was false

model:
  name: "TiViT-Piano"
  head_mode: "frame"
  tiles: 3
  input_channels: 3
  transformer:
    type: "ViViT"
    input_patch_size: 16
    tube_size: 2
    d_model: 192
    num_heads: 3
    depth_temporal: 1
    depth_spatial: 1
    depth_global: 1
    global_tokens: 2
    mlp_ratio: 3.0
    dropout: 0.20

tiling:
  patch_w: 16
  tokens_split: "auto"
  overlap_tokens: 0

training:
  epochs: 3
  learning_rate: 1e-4
  weight_decay: 0.01
  eval_freq: 1
  debug_dummy_labels: false
  log_interval: 25
  save_every: 1
  amp: false
  resume: true
  reset_head_bias: false
  bias_seed:
    onoff_prior_mean: 0.02
  loss_weights:
    #choose the on/off loss family
    onoff_loss: "bce_pos"          # "bce_pos" or "focal"
    
    #if "bce_pos":
    onoff_pos_weight_mode: "fixed"  # "adaptive" or "fixed"
    onoff_pos_weight: 20.00              # used only when mode == "fixed"
    onoff_neg_smooth: 0.00             # e.g., 0.02 if you need it
    
    # focal params (used only when onoff_loss == "focal")
    focal_gamma: 2.0
    focal_alpha: 0.25
    
    # light prior (keep 0.00 while fixing heads; re-enable later if you want)
    onoff_prior_mean: 0.02
    onoff_prior_weight: 0.10  # keep small, e.g. 0.05
    
    # per-task weights
    onset: 1.5
    offset: 1.5
    pitch: 2.0
    hand: 0.5
    clef: 0.5

  metrics:
    prob_threshold: 0.5 
    prob_threshold_onset: 0.56
    prob_threshold_offset: 0.56
    aggregation:
      mode: "k_of_p"  # "any" | "k_of_p" | "top_k_cap" | "sum_prob"
      k:
        onset: 3
        offset: 1
      top_k: 0.0
      tau_sum: 0.0

optim:
  grad_clip: 1.0

train:
  accumulate_steps: 4  #It was 1
  freeze_backbone_epochs: 0

logging:
  log_dir: "logs"
  checkpoint_dir: "checkpoints"
  tensorboard: true

inference:
  output_dir: "inference_results"
  predict_on_train: false
